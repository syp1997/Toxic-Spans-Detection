{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToxicSpans_v3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCOPRfXOyx7S"
      },
      "source": [
        "# !git clone https://github.com/ipavlopoulos/toxic_spans.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOkoyQsdzULS"
      },
      "source": [
        "# !pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nnfu24uHzXsI",
        "outputId": "cd2108ab-d415-403a-a6df-5229f31241b1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Apr 20 03:50:50 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLCJZi3by5WG"
      },
      "source": [
        "from ast import literal_eval\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnjxVa0OVNea"
      },
      "source": [
        "# set up logging\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.getLogger('transformers').setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmjKQ3cXSelg"
      },
      "source": [
        "translationTable = str.maketrans(\"éàèùaêóïü\",\"eaeuaeoiu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "9dYjJLf3y5YO",
        "outputId": "ea4e4dbc-4cb2-4183-fc66-22b1d41111b1"
      },
      "source": [
        "tsd = pd.read_csv(\"toxic_spans/data/tsd_train.csv\") \n",
        "tsd.text = tsd.text.apply(lambda x:x.translate(translationTable))\n",
        "tsd.spans = tsd.spans.apply(literal_eval)\n",
        "tsd.tail(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spans</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7934</th>\n",
              "      <td>[8, 9, 10, 11]</td>\n",
              "      <td>Another fool pipes in.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7935</th>\n",
              "      <td>[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 5...</td>\n",
              "      <td>So if a restaurant owner puts up a sign saying...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7936</th>\n",
              "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
              "      <td>Any faith that can't stand up to logic and rea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7937</th>\n",
              "      <td>[5, 6, 7, 8, 9, 10, 11]</td>\n",
              "      <td>This idiotic. Use the surplus to pay down the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7938</th>\n",
              "      <td>[106, 107, 108, 109, 110, 169, 170, 171, 172, ...</td>\n",
              "      <td>Who is this \"we\" of which you speak? Are you r...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  spans                                               text\n",
              "7934                                     [8, 9, 10, 11]                             Another fool pipes in.\n",
              "7935  [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 5...  So if a restaurant owner puts up a sign saying...\n",
              "7936  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  Any faith that can't stand up to logic and rea...\n",
              "7937                            [5, 6, 7, 8, 9, 10, 11]  This idiotic. Use the surplus to pay down the ...\n",
              "7938  [106, 107, 108, 109, 110, 169, 170, 171, 172, ...  Who is this \"we\" of which you speak? Are you r..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm6qdnG9y5ao"
      },
      "source": [
        "text_list = tsd.text.to_list()\n",
        "spans_list = tsd.spans.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EckJscETy5ck",
        "outputId": "56928438-3555-4498-8305-74afa1571260"
      },
      "source": [
        "print(len(text_list))\n",
        "print(len(spans_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7939\n",
            "7939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmMMvE4McZQO"
      },
      "source": [
        "model_name = \"bert-base-uncased\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi7nb_gjzNvr"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h49VTpefmny"
      },
      "source": [
        "# tokenizer.convert_ids_to_tokens(tokenizer(text_list[-1])['input_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkRGzg9AIFsa"
      },
      "source": [
        "special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ciQ8nVh6Paz"
      },
      "source": [
        "def find_idx(sentence, token, position=0):\n",
        "    start = sentence.find(token, position)\n",
        "    end = start + len(token)\n",
        "    if start == -1:\n",
        "        return []\n",
        "    return list(range(start,end))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcRW8DlKGeBW"
      },
      "source": [
        "def encode_and_trans_labels(text_list, spans_list):\n",
        "    inputs = tokenizer(\n",
        "        text_list,                      \n",
        "        add_special_tokens = True,             \n",
        "        truncation=True,\n",
        "        padding = 'max_length',     \n",
        "        return_tensors = 'pt',\n",
        "        max_length = 128\n",
        "    )\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    labels = []\n",
        "    for ids, sentence, toxic_spans in zip(input_ids, text_list, spans_list):\n",
        "        # print(sentence)\n",
        "        # print(toxic_spans)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(ids)\n",
        "        token_labels = []\n",
        "        position = 0\n",
        "        for token in tokens[:]:\n",
        "            if token in special_tokens.values():\n",
        "                token_labels.append(0.)\n",
        "            else:\n",
        "                token = token.replace(\"##\",\"\")\n",
        "                spans = find_idx(sentence.lower(), token, position=position)\n",
        "                if spans == []:\n",
        "                    spans = list(range(position,position+len(token)))\n",
        "                    print(\"not find:\",token,spans,position,\"\".join([sentence[i] for i in spans]))\n",
        "                # print(token,spans,position)\n",
        "                position = spans[-1]+1\n",
        "                if set(spans[:]) <= set(toxic_spans) or (set(toxic_spans)<=set(spans) and len(set(toxic_spans))>0):\n",
        "                    token_labels.append(1.)\n",
        "                elif len(set(spans[:-1])) > 0 and (set(spans[:-1]) <= set(toxic_spans)):\n",
        "                    token_labels.append(1.)\n",
        "                else:\n",
        "                    token_labels.append(0.)\n",
        "                # print(token,spans,position)\n",
        "        labels.append(token_labels)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return input_ids, attention_mask, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1cKOCLgPGV1"
      },
      "source": [
        "class TextDataSet(Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels, text_list, spans_list):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.labels = labels\n",
        "        self.text_list = text_list\n",
        "        self.spans_list = spans_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):   \n",
        "        return (self.input_ids[idx], self.attention_mask[idx], self.labels[idx], self.text_list[idx], self.spans_list[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woc4aK2DHLm-"
      },
      "source": [
        "input_ids,attention_mask,labels = encode_and_trans_labels(text_list, spans_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWPx6ernNhL7",
        "outputId": "c1f83c2b-cb51-4155-8b40-1c07263d194e"
      },
      "source": [
        "dataset = TextDataSet(input_ids, attention_mask, labels, text_list, spans_list)\n",
        "train_size = int(len(dataset)*0.8)\n",
        "valid_size = len(dataset)-train_size\n",
        "train_dataset, valid_dataset = random_split(dataset, [train_size,valid_size])\n",
        "print('Train samples: {}, Valid samples: {}'.format(len(train_dataset), len(valid_dataset)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train samples: 6351, Valid samples: 1588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r5iu0GmPJD1"
      },
      "source": [
        "import re\n",
        "import collections\n",
        "from torch._six import string_classes\n",
        "\n",
        "np_str_obj_array_pattern = re.compile(r'[SaUO]')\n",
        "\n",
        "def default_collate(batch):\n",
        "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
        "\n",
        "    elem = batch[0]\n",
        "    elem_type = type(elem)\n",
        "    if isinstance(elem, torch.Tensor):\n",
        "        out = None\n",
        "        if torch.utils.data.get_worker_info() is not None:\n",
        "            # If we're in a background process, concatenate directly into a\n",
        "            # shared memory tensor to avoid an extra copy\n",
        "            numel = sum([x.numel() for x in batch])\n",
        "            storage = elem.storage()._new_shared(numel)\n",
        "            out = elem.new(storage)\n",
        "        return torch.stack(batch, 0, out=out)\n",
        "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
        "            and elem_type.__name__ != 'string_':\n",
        "        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
        "            # array of string classes and object\n",
        "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
        "                raise TypeError(default_collate_err_msg_format.format(elem.dtype))\n",
        "\n",
        "            return default_collate([torch.as_tensor(b) for b in batch])\n",
        "        elif elem.shape == ():  # scalars\n",
        "            return torch.as_tensor(batch)\n",
        "    elif isinstance(elem, float):\n",
        "        return torch.tensor(batch, dtype=torch.float64)\n",
        "    elif isinstance(elem, int):\n",
        "        return torch.tensor(batch)\n",
        "    elif isinstance(elem, string_classes):\n",
        "        return batch\n",
        "    elif isinstance(elem, list):\n",
        "        return batch\n",
        "    elif isinstance(elem, collections.abc.Mapping):\n",
        "        return {key: default_collate([d[key] for d in batch]) for key in elem}\n",
        "    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n",
        "        return elem_type(*(default_collate(samples) for samples in zip(*batch)))\n",
        "    elif isinstance(elem, collections.abc.Sequence):\n",
        "        # check to make sure that the elements in batch have consistent size\n",
        "        it = iter(batch)\n",
        "        elem_size = len(next(it))\n",
        "        if not all(len(elem) == elem_size for elem in it):\n",
        "            raise RuntimeError('each element in list of batch should be of equal size')\n",
        "        transposed = zip(*batch)\n",
        "        return [default_collate(samples) for samples in transposed]\n",
        "\n",
        "    raise TypeError(default_collate_err_msg_format.format(elem_type))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXuDXs6LNhOa"
      },
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=default_collate)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=default_collate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_fq789fH5C3"
      },
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDYLv3fVos2o"
      },
      "source": [
        "weight_CE = torch.FloatTensor([1,5]).to(device)\n",
        "loss_fct = nn.CrossEntropyLoss(weight=weight_CE)\n",
        "def loss_fn(logits, attention_mask, labels):\n",
        "    active_loss = attention_mask.view(-1) == 1\n",
        "    active_logits = logits.view(-1, 2)\n",
        "    active_labels = torch.where(\n",
        "        active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "    )\n",
        "    loss = loss_fct(active_logits, active_labels)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiX-OyyOL4wy"
      },
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, model, train_loader, valid_loader, config):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.valid_loader = valid_loader\n",
        "        self.config = config\n",
        "\n",
        "        # take over whatever gpus are on the system\n",
        "        self.device = 'cpu'\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.cuda.current_device()\n",
        "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        # DataParallel wrappers keep raw model object in .module attribute\n",
        "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
        "        os.makedirs(self.config.ckpt_path, exist_ok=True)\n",
        "        logger.info(\"Save model to {}\".format(self.config.ckpt_path))\n",
        "        torch.save(raw_model.state_dict(), self.config.ckpt_path+\"bert_model.pt\")\n",
        "\n",
        "    def train(self):\n",
        "        model, config = self.model, self.config\n",
        "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.learning_rate, betas=config.betas)\n",
        "        def run_epoch(split):\n",
        "            is_train = (split == 'train')\n",
        "            model.train(is_train)\n",
        "            loader = self.train_loader if is_train else self.valid_loader\n",
        "            losses = []\n",
        "            spans_list_all = []\n",
        "            spans_pred_all = []\n",
        "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
        "            for it, (input_ids, attention_mask, y, text_list, spans_list) in pbar:\n",
        "                # place data on the correct device\n",
        "                input_ids = input_ids.to(self.device)\n",
        "                attention_mask = attention_mask.to(self.device)\n",
        "                y = y.to(self.device).long()\n",
        "                # forward the model\n",
        "                with torch.set_grad_enabled(is_train):\n",
        "                    outputs = model(input_ids, attention_mask, labels=y)\n",
        "                    logits = outputs.logits\n",
        "                    # loss = outputs.loss\n",
        "                    loss = loss_fn(logits, attention_mask, y)\n",
        "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
        "                    losses.append(loss.item())\n",
        "\n",
        "                    spans_pred = decode_and_trans_labels(text_list, input_ids, logits)\n",
        "                    # print(\"pred: \",spans_pred)\n",
        "                    # print(\"gold: \",spans_list)\n",
        "                    f1_score, recall_score, precision_score = batch_score(spans_pred, spans_list)\n",
        "\n",
        "                    spans_list_all.extend(spans_list)\n",
        "                    spans_pred_all.extend(spans_pred)\n",
        "\n",
        "                    # gold = (\"\".join(text_list[0][i] for i in spans_list[0])).lower()\n",
        "                    # spans_labels = torch.nonzero(y[0].cpu().detach())\n",
        "                    # spans_labels = tokenizer.convert_ids_to_tokens([input_ids[0][i] for i in spans_labels])\n",
        "                    # spans_labels = \"\".join([i.replace(\"##\",\"\") for i in spans_labels])\n",
        "                    # pred = \"\".join(text_list[0][i] for i in spans_pred[0])\n",
        "                    # print(\"\\ngold: {}\\nlables: {}\\npred: {}\".format(gold,spans_labels,pred))\n",
        "                \n",
        "                if is_train:\n",
        "\n",
        "                    # backprop and update the parameters\n",
        "                    model.zero_grad()\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # decay the learning rate based on our progress\n",
        "                    if config.lr_decay:\n",
        "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
        "                        if self.tokens < config.warmup_tokens:\n",
        "                            # linear warmup\n",
        "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
        "                        else:\n",
        "                            # cosine learning rate decay\n",
        "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
        "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "                        lr = config.learning_rate * lr_mult\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "                    else:\n",
        "                        lr = config.learning_rate\n",
        "\n",
        "                    # report progress\n",
        "                    pbar.set_description(\"epoch {} iter {}: train loss {:.5f}, f1 {:.2f}%, recall {:.2f}%, precision {:.2f}%, lr {:e}\"\\\n",
        "                                         .format(epoch+1,it,loss.item(),f1_score*100,recall_score*100,precision_score*100,lr))\n",
        "                    # pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}, f1 {f1_score:.2f}, recall_score {recall_score:.5f}, lr {lr:e}\")\n",
        "\n",
        "            if not is_train:\n",
        "                valid_loss = float(np.mean(losses))\n",
        "                valid_f1_score, valid_recall_score, valid_precision_score = batch_score(spans_pred_all, spans_list_all)\n",
        "                logger.info(\"valid loss: {:.5f}\".format(valid_loss))\n",
        "                logger.info(\"valid f1 score: {:.2f}%\".format(valid_f1_score*100))\n",
        "                logger.info(\"valid recall: {:.2f}%\".format(valid_recall_score*100))\n",
        "                logger.info(\"valid precision: {:.2f}%\".format(valid_precision_score*100))\n",
        "                return valid_loss\n",
        "\n",
        "        self.tokens = 0 # counter used for learning rate decay\n",
        "        best_loss = float('inf')\n",
        "        valid_loss = run_epoch('valid')\n",
        "        for epoch in range(config.max_epochs):\n",
        "            \n",
        "            run_epoch('train')\n",
        "            if self.valid_loader is not None:\n",
        "                valid_loss = run_epoch('valid')\n",
        "            # supports early stopping based on the valid loss, or just save always if no valid set is provided\n",
        "            good_model = self.valid_loader is None or valid_loss < best_loss\n",
        "            if self.config.ckpt_path is not None and good_model:\n",
        "                best_loss = valid_loss\n",
        "                self.save_checkpoint()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCvFh5tyQx4e"
      },
      "source": [
        "def decode_and_trans_labels(text_list, input_ids, logits):\n",
        "    token_labels = torch.argmax(logits, dim=-1).cpu().detach().numpy()\n",
        "    spans_pred = []\n",
        "    for ids, sentence, labels in zip(input_ids, text_list, token_labels):\n",
        "        tokens = tokenizer.convert_ids_to_tokens(ids)\n",
        "        position = 0\n",
        "        toxic_spans = []\n",
        "        # print(sentence)\n",
        "        for token, label in zip(tokens, labels):\n",
        "            if token in special_tokens.values():\n",
        "                continue\n",
        "            token = token.replace(\"##\",\"\")\n",
        "            spans = find_idx(sentence.lower(), token, position=position)\n",
        "            # print(token,spans)\n",
        "            if spans == []:\n",
        "                spans = list(range(position,position+len(token)))\n",
        "                print(\"not find:\",token,spans,position,\"\".join([sentence[i] for i in spans]))\n",
        "            position = spans[-1]+1\n",
        "            if label == 1:\n",
        "                toxic_spans.extend(spans)\n",
        "        spans_pred.append(toxic_spans)\n",
        "    return spans_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQmYuB_fQzwk"
      },
      "source": [
        "def f1(predictions, gold):\n",
        "    \"\"\"\n",
        "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
        "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
        "    :param predictions: a list of predicted offsets\n",
        "    :param gold: a list of offsets serving as the ground truth\n",
        "    :return: a score between 0 and 1\n",
        "    \"\"\"\n",
        "    if len(gold) == 0:\n",
        "        return 1. if len(predictions) == 0 else 0.\n",
        "    if len(predictions) == 0:\n",
        "        return 0.\n",
        "    predictions_set = set(predictions)\n",
        "    gold_set = set(gold)\n",
        "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
        "    denom = len(predictions_set) + len(gold_set)\n",
        "    return float(nom)/float(denom)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h0PRxuasMmd"
      },
      "source": [
        "def recall(predictions, gold):\n",
        "    if len(gold) == 0:\n",
        "        return 1. if len(predictions) == 0 else 0.\n",
        "    if len(predictions) == 0:\n",
        "        return 0.\n",
        "    predictions_set = set(predictions)\n",
        "    gold_set = set(gold)\n",
        "    nom = len(predictions_set.intersection(gold_set))\n",
        "    denom = len(gold_set)\n",
        "    return float(nom)/float(denom)\n",
        "\n",
        "def precision(predictions, gold):\n",
        "    if len(gold) == 0:\n",
        "        return 1. if len(predictions) == 0 else 0.\n",
        "    if len(predictions) == 0:\n",
        "        return 0.\n",
        "    predictions_set = set(predictions)\n",
        "    gold_set = set(gold)\n",
        "    nom = len(predictions_set.intersection(gold_set))\n",
        "    denom = len(predictions_set)\n",
        "    return float(nom)/float(denom)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwbwaKD_Q0U1"
      },
      "source": [
        "def batch_score(spans_pred, spans_list):\n",
        "    f1_scores = []\n",
        "    recall_scores = []\n",
        "    precision_scores = []\n",
        "    for pred, gold in zip(spans_pred, spans_list):\n",
        "        f1_score = f1(pred, gold)\n",
        "        recall_score = recall(pred, gold)\n",
        "        precision_score = precision(pred, gold)\n",
        "        f1_scores.append(f1_score)\n",
        "        recall_scores.append(recall_score)\n",
        "        precision_scores.append(precision_score)\n",
        "    return np.mean(f1_scores), np.mean(recall_scores), np.mean(precision_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yZp_pa5L4y6"
      },
      "source": [
        "class TrainerConfig:\n",
        "    # optimization parameters\n",
        "    max_epochs = 10\n",
        "    learning_rate = 1e-5\n",
        "    betas = (0.9, 0.95)\n",
        "    grad_norm_clip = 1.0\n",
        "    weight_decay = 0.1 # may useful optimize method\n",
        "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
        "    lr_decay = False # optimize method\n",
        "    warmup_tokens = 375e6 # use this to train model from a lower learning rate\n",
        "    final_tokens = 260e9 # all tokens during whole training process\n",
        "    # checkpoint settings\n",
        "    ckpt_path = './models/' # save model path\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k,v in kwargs.items():\n",
        "            print(k,v)\n",
        "            setattr(self, k, v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nv5Vk-UL41J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf26d743-b042-40da-b155-45581e60e99e"
      },
      "source": [
        "# print model all parameters and parameters need training\n",
        "print('{} : all params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters()) / 1000 / 1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BertForTokenClassification : all params: 108.893186M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRl2WNjAL43U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15a7b93c-af61-4a1c-ce10-5df291851e0d"
      },
      "source": [
        "max_epochs = 8\n",
        "final_tokens = max_epochs * batch_size * len(train_loader)\n",
        "warmup_tokens = final_tokens//10\n",
        "tconf = TrainerConfig(max_epochs=max_epochs, learning_rate=1e-5, lr_decay=True, \n",
        "                      warmup_tokens=warmup_tokens, final_tokens=final_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_epochs 8\n",
            "learning_rate 1e-05\n",
            "lr_decay True\n",
            "warmup_tokens 5094\n",
            "final_tokens 50944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL4rnw7ZL45R"
      },
      "source": [
        "trainer = Trainer(model, train_loader, valid_loader, tconf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h49l2wyeL47g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dda13ff-df8b-4839-c98d-309b113b7f58"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/20/2021 03:51:11 - valid loss: 0.71996\n",
            "04/20/2021 03:51:11 - valid f1 score: 10.68%\n",
            "04/20/2021 03:51:11 - valid recall: 20.95%\n",
            "04/20/2021 03:51:11 - valid precision: 10.99%\n",
            "epoch 1 iter 198: train loss 0.33016, f1 36.66%, recall 57.83%, precision 38.73%, lr 6.826918e-06: 100%|██████████| 199/199 [01:29<00:00,  2.22it/s]\n",
            "04/20/2021 03:52:50 - valid loss: 0.44170\n",
            "04/20/2021 03:52:50 - valid f1 score: 54.97%\n",
            "04/20/2021 03:52:50 - valid recall: 75.65%\n",
            "04/20/2021 03:52:50 - valid precision: 52.48%\n",
            "04/20/2021 03:52:50 - Save model to ./models/\n",
            "epoch 2 iter 198: train loss 0.92816, f1 54.65%, recall 66.26%, precision 63.89%, lr 2.719218e-06: 100%|██████████| 199/199 [01:29<00:00,  2.22it/s]\n",
            "04/20/2021 03:54:30 - valid loss: 0.42776\n",
            "04/20/2021 03:54:30 - valid f1 score: 57.79%\n",
            "04/20/2021 03:54:30 - valid recall: 76.25%\n",
            "04/20/2021 03:54:30 - valid precision: 56.21%\n",
            "04/20/2021 03:54:30 - Save model to ./models/\n",
            "epoch 3 iter 198: train loss 0.56878, f1 46.33%, recall 72.84%, precision 45.67%, lr 1.000000e-06: 100%|██████████| 199/199 [01:29<00:00,  2.22it/s]\n",
            "04/20/2021 03:56:09 - valid loss: 0.42417\n",
            "04/20/2021 03:56:09 - valid f1 score: 61.11%\n",
            "04/20/2021 03:56:09 - valid recall: 76.47%\n",
            "04/20/2021 03:56:09 - valid precision: 60.85%\n",
            "04/20/2021 03:56:09 - Save model to ./models/\n",
            "epoch 4 iter 198: train loss 0.30855, f1 54.88%, recall 81.58%, precision 51.12%, lr 1.000000e-06: 100%|██████████| 199/199 [01:29<00:00,  2.22it/s]\n",
            "04/20/2021 03:57:49 - valid loss: 0.43741\n",
            "04/20/2021 03:57:49 - valid f1 score: 52.60%\n",
            "04/20/2021 03:57:49 - valid recall: 80.40%\n",
            "04/20/2021 03:57:49 - valid precision: 47.60%\n",
            "epoch 5 iter 198: train loss 0.41068, f1 51.74%, recall 64.57%, precision 51.33%, lr 4.377753e-06: 100%|██████████| 199/199 [01:29<00:00,  2.22it/s]\n",
            "04/20/2021 03:59:28 - valid loss: 0.43459\n",
            "04/20/2021 03:59:28 - valid f1 score: 53.65%\n",
            "04/20/2021 03:59:28 - valid recall: 80.91%\n",
            "04/20/2021 03:59:28 - valid precision: 49.24%\n",
            "epoch 6 iter 198: train loss 0.15282, f1 58.64%, recall 72.56%, precision 58.03%, lr 8.307973e-06: 100%|██████████| 199/199 [01:29<00:00,  2.22it/s]\n",
            "04/20/2021 04:01:06 - valid loss: 0.46137\n",
            "04/20/2021 04:01:06 - valid f1 score: 59.12%\n",
            "04/20/2021 04:01:06 - valid recall: 78.43%\n",
            "04/20/2021 04:01:06 - valid precision: 57.62%\n",
            "epoch 7 iter 198: train loss 0.23584, f1 74.88%, recall 82.52%, precision 75.81%, lr 1.000000e-05: 100%|██████████| 199/199 [01:29<00:00,  2.22it/s]\n",
            "04/20/2021 04:02:45 - valid loss: 0.51454\n",
            "04/20/2021 04:02:45 - valid f1 score: 63.30%\n",
            "04/20/2021 04:02:45 - valid recall: 75.31%\n",
            "04/20/2021 04:02:45 - valid precision: 65.26%\n",
            "epoch 8 iter 198: train loss 0.32719, f1 66.38%, recall 66.79%, precision 71.44%, lr 8.309000e-06: 100%|██████████| 199/199 [01:29<00:00,  2.22it/s]\n",
            "04/20/2021 04:04:23 - valid loss: 0.52038\n",
            "04/20/2021 04:04:23 - valid f1 score: 59.04%\n",
            "04/20/2021 04:04:23 - valid recall: 77.42%\n",
            "04/20/2021 04:04:23 - valid precision: 58.06%\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pidTsbGNtp77"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCT3YaNatp9_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWkr_vsTWyWU"
      },
      "source": [
        "## On Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VigQ7VwtqAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf5f227c-848c-4589-c6a2-18e8b4625d93"
      },
      "source": [
        "model.load_state_dict(torch.load('models/bert_model.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "SM9r3TrRW1_k",
        "outputId": "a5d369fd-b9e5-470f-feef-54e9e2dabd26"
      },
      "source": [
        "tsd_test = pd.read_csv(\"toxic_spans/data/tsd_test.csv\") \n",
        "tsd_test.text = tsd_test.text.apply(lambda x:x.translate(translationTable))\n",
        "tsd_test.spans = tsd_test.spans.apply(literal_eval)\n",
        "tsd_test.tail(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spans</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>[4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...</td>\n",
              "      <td>hey loser change your name to something more a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>[23, 24, 25, 26, 27]</td>\n",
              "      <td>And you are a complete moron who obviously doe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>[157, 158, 159, 160, 161, 162, 163, 164, 165, ...</td>\n",
              "      <td>Such vitriol from the left.  Who would have th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>[]</td>\n",
              "      <td>It is now time for most of you to expand your ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>[828, 829, 830, 831]</td>\n",
              "      <td>Why does this author think she can demand, or ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  spans                                               text\n",
              "1995  [4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...  hey loser change your name to something more a...\n",
              "1996                               [23, 24, 25, 26, 27]  And you are a complete moron who obviously doe...\n",
              "1997  [157, 158, 159, 160, 161, 162, 163, 164, 165, ...  Such vitriol from the left.  Who would have th...\n",
              "1998                                                 []  It is now time for most of you to expand your ...\n",
              "1999                               [828, 829, 830, 831]  Why does this author think she can demand, or ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fRVTKdWW2C9",
        "outputId": "7446d05a-45bf-4bfd-ecad-b2eeeeb9bc62"
      },
      "source": [
        "text_list_test = tsd_test.text.to_list()\n",
        "spans_list_test = tsd_test.spans.to_list()\n",
        "print(len(text_list_test))\n",
        "print(len(spans_list_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xtsw5sftZhW"
      },
      "source": [
        "def predict(text,spans):\n",
        "    input_ids,attention_mask,labels = encode_and_trans_labels([text], [spans])\n",
        "    outputs = model(input_ids.to(device), attention_mask.to(device), labels=labels.to(device).long())\n",
        "    logits = outputs.logits \n",
        "    spans_pred = decode_and_trans_labels([text], input_ids, logits)[0]\n",
        "    gold = \"\".join(text[i] for i in spans)\n",
        "    pred = \"\".join(text[i] for i in spans_pred)\n",
        "    # print(\"gold: {}\\npred: {}\".format(gold,pred))\n",
        "    f1_score = f1(spans_pred, spans)\n",
        "    recall_score = recall(spans_pred, spans)\n",
        "    precision_score = precision(spans_pred, spans)\n",
        "    # print(\"f1 {:.2f}%, recall {:.2f}%, precision {:.2f}%,\".format(f1_score*100,recall_score*100,precision_score*100))\n",
        "    return f1_score,recall_score,precision_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BCuXUUPtZjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e061f30c-54d0-4a1e-f8ab-eaf0488b2376"
      },
      "source": [
        "f1_scores,recall_scores,precision_scores = [],[],[]\n",
        "for i in range(len(text_list_test)):\n",
        "    f1_score,recall_score,precision_score = predict(text_list[i],spans_list[i])\n",
        "    f1_scores.append(f1_score)\n",
        "    recall_scores.append(recall_score)\n",
        "    precision_scores.append(precision_score)\n",
        "print(\"All: f1 {:.2f}%, recall {:.2f}%, precision {:.2f}%,\".format(np.mean(f1_scores)*100,np.mean(recall_scores)*100,np.mean(precision_scores)*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All: f1 61.79%, recall 76.73%, precision 61.44%,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE-ja7Kmx_kf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUojR4tvx_mq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}