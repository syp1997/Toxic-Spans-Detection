{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ToxicSpans_v3_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCOPRfXOyx7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1f396b2-8d32-4e8c-edab-3c4a5fc53bed"
      },
      "source": [
        "!git clone https://github.com/ipavlopoulos/toxic_spans.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'toxic_spans' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOkoyQsdzULS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835bcf35-dade-42db-cb7d-f22576823abe"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nnfu24uHzXsI",
        "outputId": "91efd5df-14c6-4123-c21a-48566baeca56"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Apr 20 04:27:40 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLCJZi3by5WG"
      },
      "source": [
        "from ast import literal_eval\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import os\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'   "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnjxVa0OVNea"
      },
      "source": [
        "# set up logging\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.getLogger('transformers').setLevel(logging.ERROR)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmjKQ3cXSelg"
      },
      "source": [
        "translationTable = str.maketrans(\"éàèùaêóïü\",\"eaeuaeoiu\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "9dYjJLf3y5YO",
        "outputId": "f8b61c80-0084-4aaf-eaac-2ca67e515ffc"
      },
      "source": [
        "tsd = pd.read_csv(\"toxic_spans/data/tsd_train.csv\") \n",
        "tsd.text = tsd.text.apply(lambda x:x.translate(translationTable))\n",
        "tsd.spans = tsd.spans.apply(literal_eval)\n",
        "tsd.tail(5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spans</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7934</th>\n",
              "      <td>[8, 9, 10, 11]</td>\n",
              "      <td>Another fool pipes in.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7935</th>\n",
              "      <td>[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 5...</td>\n",
              "      <td>So if a restaurant owner puts up a sign saying...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7936</th>\n",
              "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
              "      <td>Any faith that can't stand up to logic and rea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7937</th>\n",
              "      <td>[5, 6, 7, 8, 9, 10, 11]</td>\n",
              "      <td>This idiotic. Use the surplus to pay down the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7938</th>\n",
              "      <td>[106, 107, 108, 109, 110, 169, 170, 171, 172, ...</td>\n",
              "      <td>Who is this \"we\" of which you speak? Are you r...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  spans                                               text\n",
              "7934                                     [8, 9, 10, 11]                             Another fool pipes in.\n",
              "7935  [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 5...  So if a restaurant owner puts up a sign saying...\n",
              "7936  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...  Any faith that can't stand up to logic and rea...\n",
              "7937                            [5, 6, 7, 8, 9, 10, 11]  This idiotic. Use the surplus to pay down the ...\n",
              "7938  [106, 107, 108, 109, 110, 169, 170, 171, 172, ...  Who is this \"we\" of which you speak? Are you r..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm6qdnG9y5ao"
      },
      "source": [
        "text_list = tsd.text.to_list()\n",
        "spans_list = tsd.spans.to_list()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EckJscETy5ck",
        "outputId": "e9801aa5-6631-42c3-ba7f-5e1efae7228b"
      },
      "source": [
        "print(len(text_list))\n",
        "print(len(spans_list))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7939\n",
            "7939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmMMvE4McZQO"
      },
      "source": [
        "model_name = \"bert-base-uncased\""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi7nb_gjzNvr"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h49VTpefmny"
      },
      "source": [
        "# tokenizer.convert_ids_to_tokens(tokenizer(text_list[-1])['input_ids'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkRGzg9AIFsa"
      },
      "source": [
        "special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ciQ8nVh6Paz"
      },
      "source": [
        "def find_idx(sentence, token, position=0):\n",
        "    start = sentence.find(token, position)\n",
        "    end = start + len(token)\n",
        "    if start == -1:\n",
        "        return []\n",
        "    return list(range(start,end))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcRW8DlKGeBW"
      },
      "source": [
        "def encode_and_trans_labels(text_list, spans_list):\n",
        "    inputs = tokenizer(\n",
        "        text_list,                      \n",
        "        add_special_tokens = True,             \n",
        "        truncation=True,\n",
        "        padding = 'max_length',     \n",
        "        return_tensors = 'pt',\n",
        "        max_length = 128\n",
        "    )\n",
        "    input_ids = inputs['input_ids']\n",
        "    attention_mask = inputs['attention_mask']\n",
        "\n",
        "    labels = []\n",
        "    for ids, sentence, toxic_spans in zip(input_ids, text_list, spans_list):\n",
        "        # print(sentence)\n",
        "        # print(toxic_spans)\n",
        "        tokens = tokenizer.convert_ids_to_tokens(ids)\n",
        "        token_labels = []\n",
        "        position = 0\n",
        "        for token in tokens[:]:\n",
        "            if token in special_tokens.values():\n",
        "                token_labels.append(0.)\n",
        "            else:\n",
        "                token = token.replace(\"##\",\"\")\n",
        "                spans = find_idx(sentence.lower(), token, position=position)\n",
        "                if spans == []:\n",
        "                    spans = list(range(position,position+len(token)))\n",
        "                    print(\"not find:\",token,spans,position,\"\".join([sentence[i] for i in spans]))\n",
        "                # print(token,spans,position)\n",
        "                position = spans[-1]+1\n",
        "                if set(spans[:]) <= set(toxic_spans) or (set(toxic_spans)<=set(spans) and len(set(toxic_spans))>0):\n",
        "                    token_labels.append(1.)\n",
        "                elif len(set(spans[:-1])) > 0 and (set(spans[:-1]) <= set(toxic_spans)):\n",
        "                    token_labels.append(1.)\n",
        "                else:\n",
        "                    token_labels.append(0.)\n",
        "                # print(token,spans,position)\n",
        "        labels.append(token_labels)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return input_ids, attention_mask, labels"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1cKOCLgPGV1"
      },
      "source": [
        "class TextDataSet(Dataset):\n",
        "    def __init__(self, input_ids, attention_mask, labels, text_list, spans_list):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.labels = labels\n",
        "        self.text_list = text_list\n",
        "        self.spans_list = spans_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):   \n",
        "        return (self.input_ids[idx], self.attention_mask[idx], self.labels[idx], self.text_list[idx], self.spans_list[idx])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woc4aK2DHLm-"
      },
      "source": [
        "input_ids,attention_mask,labels = encode_and_trans_labels(text_list, spans_list)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWPx6ernNhL7",
        "outputId": "24be88e0-ab8c-419a-a39c-1dcea57ccca6"
      },
      "source": [
        "dataset = TextDataSet(input_ids, attention_mask, labels, text_list, spans_list)\n",
        "train_size = int(len(dataset)*0.8)\n",
        "valid_size = len(dataset)-train_size\n",
        "train_dataset, valid_dataset = random_split(dataset, [train_size,valid_size])\n",
        "print('Train samples: {}, Valid samples: {}'.format(len(train_dataset), len(valid_dataset)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train samples: 6351, Valid samples: 1588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r5iu0GmPJD1"
      },
      "source": [
        "import re\n",
        "import collections\n",
        "from torch._six import string_classes\n",
        "\n",
        "np_str_obj_array_pattern = re.compile(r'[SaUO]')\n",
        "\n",
        "def default_collate(batch):\n",
        "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
        "\n",
        "    elem = batch[0]\n",
        "    elem_type = type(elem)\n",
        "    if isinstance(elem, torch.Tensor):\n",
        "        out = None\n",
        "        if torch.utils.data.get_worker_info() is not None:\n",
        "            # If we're in a background process, concatenate directly into a\n",
        "            # shared memory tensor to avoid an extra copy\n",
        "            numel = sum([x.numel() for x in batch])\n",
        "            storage = elem.storage()._new_shared(numel)\n",
        "            out = elem.new(storage)\n",
        "        return torch.stack(batch, 0, out=out)\n",
        "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
        "            and elem_type.__name__ != 'string_':\n",
        "        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
        "            # array of string classes and object\n",
        "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
        "                raise TypeError(default_collate_err_msg_format.format(elem.dtype))\n",
        "\n",
        "            return default_collate([torch.as_tensor(b) for b in batch])\n",
        "        elif elem.shape == ():  # scalars\n",
        "            return torch.as_tensor(batch)\n",
        "    elif isinstance(elem, float):\n",
        "        return torch.tensor(batch, dtype=torch.float64)\n",
        "    elif isinstance(elem, int):\n",
        "        return torch.tensor(batch)\n",
        "    elif isinstance(elem, string_classes):\n",
        "        return batch\n",
        "    elif isinstance(elem, list):\n",
        "        return batch\n",
        "    elif isinstance(elem, collections.abc.Mapping):\n",
        "        return {key: default_collate([d[key] for d in batch]) for key in elem}\n",
        "    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n",
        "        return elem_type(*(default_collate(samples) for samples in zip(*batch)))\n",
        "    elif isinstance(elem, collections.abc.Sequence):\n",
        "        # check to make sure that the elements in batch have consistent size\n",
        "        it = iter(batch)\n",
        "        elem_size = len(next(it))\n",
        "        if not all(len(elem) == elem_size for elem in it):\n",
        "            raise RuntimeError('each element in list of batch should be of equal size')\n",
        "        transposed = zip(*batch)\n",
        "        return [default_collate(samples) for samples in transposed]\n",
        "\n",
        "    raise TypeError(default_collate_err_msg_format.format(elem_type))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXuDXs6LNhOa"
      },
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=default_collate)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=default_collate)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_fq789fH5C3"
      },
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=2)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDYLv3fVos2o"
      },
      "source": [
        "weight_CE = torch.FloatTensor([1,2]).to(device)\n",
        "loss_fct = nn.CrossEntropyLoss(weight=weight_CE)\n",
        "def loss_fn(logits, attention_mask, labels):\n",
        "    active_loss = attention_mask.view(-1) == 1\n",
        "    active_logits = logits.view(-1, 2)\n",
        "    active_labels = torch.where(\n",
        "        active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "    )\n",
        "    loss = loss_fct(active_logits, active_labels)\n",
        "    return loss"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiX-OyyOL4wy"
      },
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, model, train_loader, valid_loader, config):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.valid_loader = valid_loader\n",
        "        self.config = config\n",
        "\n",
        "        # take over whatever gpus are on the system\n",
        "        self.device = 'cpu'\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.cuda.current_device()\n",
        "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        # DataParallel wrappers keep raw model object in .module attribute\n",
        "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
        "        os.makedirs(self.config.ckpt_path, exist_ok=True)\n",
        "        logger.info(\"Save model to {}\".format(self.config.ckpt_path))\n",
        "        torch.save(raw_model.state_dict(), self.config.ckpt_path+\"bert_model.pt\")\n",
        "\n",
        "    def train(self):\n",
        "        model, config = self.model, self.config\n",
        "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=config.learning_rate, betas=config.betas)\n",
        "        def run_epoch(split):\n",
        "            is_train = (split == 'train')\n",
        "            model.train(is_train)\n",
        "            loader = self.train_loader if is_train else self.valid_loader\n",
        "            losses = []\n",
        "            spans_list_all = []\n",
        "            spans_pred_all = []\n",
        "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
        "            for it, (input_ids, attention_mask, y, text_list, spans_list) in pbar:\n",
        "                # place data on the correct device\n",
        "                input_ids = input_ids.to(self.device)\n",
        "                attention_mask = attention_mask.to(self.device)\n",
        "                y = y.to(self.device).long()\n",
        "                # forward the model\n",
        "                with torch.set_grad_enabled(is_train):\n",
        "                    outputs = model(input_ids, attention_mask, labels=y)\n",
        "                    logits = outputs.logits\n",
        "                    # loss = outputs.loss\n",
        "                    loss = loss_fn(logits, attention_mask, y)\n",
        "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
        "                    losses.append(loss.item())\n",
        "\n",
        "                    spans_pred = decode_and_trans_labels(text_list, input_ids, logits)\n",
        "                    # print(\"pred: \",spans_pred)\n",
        "                    # print(\"gold: \",spans_list)\n",
        "                    f1_score, recall_score, precision_score = batch_score(spans_pred, spans_list)\n",
        "\n",
        "                    spans_list_all.extend(spans_list)\n",
        "                    spans_pred_all.extend(spans_pred)\n",
        "\n",
        "                    # gold = (\"\".join(text_list[0][i] for i in spans_list[0])).lower()\n",
        "                    # spans_labels = torch.nonzero(y[0].cpu().detach())\n",
        "                    # spans_labels = tokenizer.convert_ids_to_tokens([input_ids[0][i] for i in spans_labels])\n",
        "                    # spans_labels = \"\".join([i.replace(\"##\",\"\") for i in spans_labels])\n",
        "                    # pred = \"\".join(text_list[0][i] for i in spans_pred[0])\n",
        "                    # print(\"\\ngold: {}\\nlables: {}\\npred: {}\".format(gold,spans_labels,pred))\n",
        "                \n",
        "                if is_train:\n",
        "\n",
        "                    # backprop and update the parameters\n",
        "                    model.zero_grad()\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "                    optimizer.step()\n",
        "\n",
        "                    # decay the learning rate based on our progress\n",
        "                    if config.lr_decay:\n",
        "                        self.tokens += batch_size # number of tokens processed this step (i.e. label is not -100)\n",
        "                        if self.tokens < config.warmup_tokens:\n",
        "                            # linear warmup\n",
        "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
        "                        else:\n",
        "                            # cosine learning rate decay\n",
        "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
        "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "                        lr = config.learning_rate * lr_mult\n",
        "                        for param_group in optimizer.param_groups:\n",
        "                            param_group['lr'] = lr\n",
        "                    else:\n",
        "                        lr = config.learning_rate\n",
        "\n",
        "                    # report progress\n",
        "                    pbar.set_description(\"epoch {} iter {}: train loss {:.5f}, f1 {:.2f}%, recall {:.2f}%, precision {:.2f}%, lr {:e}\"\\\n",
        "                                         .format(epoch+1,it,loss.item(),f1_score*100,recall_score*100,precision_score*100,lr))\n",
        "                    # pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}, f1 {f1_score:.2f}, recall_score {recall_score:.5f}, lr {lr:e}\")\n",
        "\n",
        "            if not is_train:\n",
        "                valid_loss = float(np.mean(losses))\n",
        "                valid_f1_score, valid_recall_score, valid_precision_score = batch_score(spans_pred_all, spans_list_all)\n",
        "                logger.info(\"valid loss: {:.5f}\".format(valid_loss))\n",
        "                logger.info(\"valid f1 score: {:.2f}%\".format(valid_f1_score*100))\n",
        "                logger.info(\"valid recall: {:.2f}%\".format(valid_recall_score*100))\n",
        "                logger.info(\"valid precision: {:.2f}%\".format(valid_precision_score*100))\n",
        "                return valid_loss\n",
        "\n",
        "        self.tokens = 0 # counter used for learning rate decay\n",
        "        best_loss = float('inf')\n",
        "        valid_loss = run_epoch('valid')\n",
        "        for epoch in range(config.max_epochs):\n",
        "            \n",
        "            run_epoch('train')\n",
        "            if self.valid_loader is not None:\n",
        "                valid_loss = run_epoch('valid')\n",
        "            # supports early stopping based on the valid loss, or just save always if no valid set is provided\n",
        "            good_model = self.valid_loader is None or valid_loss < best_loss\n",
        "            if self.config.ckpt_path is not None and good_model:\n",
        "                best_loss = valid_loss\n",
        "                self.save_checkpoint()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCvFh5tyQx4e"
      },
      "source": [
        "def decode_and_trans_labels(text_list, input_ids, logits):\n",
        "    token_labels = torch.argmax(logits, dim=-1).cpu().detach().numpy()\n",
        "    spans_pred = []\n",
        "    for ids, sentence, labels in zip(input_ids, text_list, token_labels):\n",
        "        tokens = tokenizer.convert_ids_to_tokens(ids)\n",
        "        position = 0\n",
        "        toxic_spans = []\n",
        "        # print(sentence)\n",
        "        for token, label in zip(tokens, labels):\n",
        "            if token in special_tokens.values():\n",
        "                continue\n",
        "            token = token.replace(\"##\",\"\")\n",
        "            spans = find_idx(sentence.lower(), token, position=position)\n",
        "            # print(token,spans)\n",
        "            if spans == []:\n",
        "                spans = list(range(position,position+len(token)))\n",
        "                print(\"not find:\",token,spans,position,\"\".join([sentence[i] for i in spans]))\n",
        "            position = spans[-1]+1\n",
        "            if label == 1:\n",
        "                toxic_spans.extend(spans)\n",
        "        spans_pred.append(toxic_spans)\n",
        "    return spans_pred"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQmYuB_fQzwk"
      },
      "source": [
        "def f1(predictions, gold):\n",
        "    \"\"\"\n",
        "    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n",
        "    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n",
        "    :param predictions: a list of predicted offsets\n",
        "    :param gold: a list of offsets serving as the ground truth\n",
        "    :return: a score between 0 and 1\n",
        "    \"\"\"\n",
        "    if len(gold) == 0:\n",
        "        return 1. if len(predictions) == 0 else 0.\n",
        "    if len(predictions) == 0:\n",
        "        return 0.\n",
        "    predictions_set = set(predictions)\n",
        "    gold_set = set(gold)\n",
        "    nom = 2 * len(predictions_set.intersection(gold_set))\n",
        "    denom = len(predictions_set) + len(gold_set)\n",
        "    return float(nom)/float(denom)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h0PRxuasMmd"
      },
      "source": [
        "def recall(predictions, gold):\n",
        "    if len(gold) == 0:\n",
        "        return 1. if len(predictions) == 0 else 0.\n",
        "    if len(predictions) == 0:\n",
        "        return 0.\n",
        "    predictions_set = set(predictions)\n",
        "    gold_set = set(gold)\n",
        "    nom = len(predictions_set.intersection(gold_set))\n",
        "    denom = len(gold_set)\n",
        "    return float(nom)/float(denom)\n",
        "\n",
        "def precision(predictions, gold):\n",
        "    if len(gold) == 0:\n",
        "        return 1. if len(predictions) == 0 else 0.\n",
        "    if len(predictions) == 0:\n",
        "        return 0.\n",
        "    predictions_set = set(predictions)\n",
        "    gold_set = set(gold)\n",
        "    nom = len(predictions_set.intersection(gold_set))\n",
        "    denom = len(predictions_set)\n",
        "    return float(nom)/float(denom)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwbwaKD_Q0U1"
      },
      "source": [
        "def batch_score(spans_pred, spans_list):\n",
        "    f1_scores = []\n",
        "    recall_scores = []\n",
        "    precision_scores = []\n",
        "    for pred, gold in zip(spans_pred, spans_list):\n",
        "        f1_score = f1(pred, gold)\n",
        "        recall_score = recall(pred, gold)\n",
        "        precision_score = precision(pred, gold)\n",
        "        f1_scores.append(f1_score)\n",
        "        recall_scores.append(recall_score)\n",
        "        precision_scores.append(precision_score)\n",
        "    return np.mean(f1_scores), np.mean(recall_scores), np.mean(precision_scores)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yZp_pa5L4y6"
      },
      "source": [
        "class TrainerConfig:\n",
        "    # optimization parameters\n",
        "    max_epochs = 10\n",
        "    learning_rate = 1e-5\n",
        "    betas = (0.9, 0.95)\n",
        "    grad_norm_clip = 1.0\n",
        "    weight_decay = 0.1 # may useful optimize method\n",
        "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
        "    lr_decay = False # optimize method\n",
        "    warmup_tokens = 375e6 # use this to train model from a lower learning rate\n",
        "    final_tokens = 260e9 # all tokens during whole training process\n",
        "    # checkpoint settings\n",
        "    ckpt_path = './models/' # save model path\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k,v in kwargs.items():\n",
        "            print(k,v)\n",
        "            setattr(self, k, v)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nv5Vk-UL41J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc161a51-8e93-451f-abfc-f2640378ecf3"
      },
      "source": [
        "# print model all parameters and parameters need training\n",
        "print('{} : all params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters()) / 1000 / 1000))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BertForTokenClassification : all params: 108.893186M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRl2WNjAL43U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beb44dda-dbae-4d82-96dd-2bc5c0ef5295"
      },
      "source": [
        "max_epochs = 6\n",
        "final_tokens = max_epochs * batch_size * len(train_loader)\n",
        "warmup_tokens = final_tokens//10\n",
        "tconf = TrainerConfig(max_epochs=max_epochs, learning_rate=1e-5, lr_decay=True, \n",
        "                      warmup_tokens=warmup_tokens, final_tokens=final_tokens)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_epochs 6\n",
            "learning_rate 1e-05\n",
            "lr_decay True\n",
            "warmup_tokens 3820\n",
            "final_tokens 38208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL4rnw7ZL45R"
      },
      "source": [
        "trainer = Trainer(model, train_loader, valid_loader, tconf)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h49l2wyeL47g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b462dca-54d0-4f65-9c40-c75682d7d16c"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04/20/2021 04:28:02 - valid loss: 0.61711\n",
            "04/20/2021 04:28:02 - valid f1 score: 20.55%\n",
            "04/20/2021 04:28:02 - valid recall: 43.82%\n",
            "04/20/2021 04:28:02 - valid precision: 18.47%\n",
            "epoch 1 iter 198: train loss 0.15111, f1 63.93%, recall 61.61%, precision 80.00%, lr 9.865146e-06: 100%|██████████| 199/199 [01:29<00:00,  2.23it/s]\n",
            "04/20/2021 04:29:40 - valid loss: 0.31078\n",
            "04/20/2021 04:29:40 - valid f1 score: 60.69%\n",
            "04/20/2021 04:29:40 - valid recall: 64.59%\n",
            "04/20/2021 04:29:40 - valid precision: 67.45%\n",
            "04/20/2021 04:29:40 - Save model to ./models/\n",
            "epoch 2 iter 198: train loss 0.27882, f1 72.61%, recall 77.86%, precision 79.13%, lr 8.431011e-06: 100%|██████████| 199/199 [01:29<00:00,  2.23it/s]\n",
            "04/20/2021 04:31:20 - valid loss: 0.30185\n",
            "04/20/2021 04:31:20 - valid f1 score: 63.88%\n",
            "04/20/2021 04:31:20 - valid recall: 70.63%\n",
            "04/20/2021 04:31:20 - valid precision: 68.97%\n",
            "04/20/2021 04:31:20 - Save model to ./models/\n",
            "epoch 3 iter 198: train loss 0.16657, f1 68.92%, recall 76.94%, precision 72.79%, lr 5.868041e-06: 100%|██████████| 199/199 [01:29<00:00,  2.23it/s]\n",
            "04/20/2021 04:32:59 - valid loss: 0.30313\n",
            "04/20/2021 04:32:59 - valid f1 score: 65.06%\n",
            "04/20/2021 04:32:59 - valid recall: 69.94%\n",
            "04/20/2021 04:32:59 - valid precision: 71.46%\n",
            "epoch 4 iter 198: train loss 0.15589, f1 79.87%, recall 85.85%, precision 80.70%, lr 3.019477e-06: 100%|██████████| 199/199 [01:29<00:00,  2.22it/s]\n",
            "04/20/2021 04:34:37 - valid loss: 0.29766\n",
            "04/20/2021 04:34:37 - valid f1 score: 64.92%\n",
            "04/20/2021 04:34:37 - valid recall: 72.26%\n",
            "04/20/2021 04:34:37 - valid precision: 69.42%\n",
            "04/20/2021 04:34:37 - Save model to ./models/\n",
            "epoch 5 iter 198: train loss 0.20511, f1 74.11%, recall 71.95%, precision 82.28%, lr 1.000000e-06: 100%|██████████| 199/199 [01:29<00:00,  2.22it/s]\n",
            "04/20/2021 04:36:17 - valid loss: 0.31511\n",
            "04/20/2021 04:36:17 - valid f1 score: 65.35%\n",
            "04/20/2021 04:36:17 - valid recall: 71.66%\n",
            "04/20/2021 04:36:17 - valid precision: 70.56%\n",
            "epoch 6 iter 198: train loss 0.19220, f1 63.73%, recall 63.00%, precision 72.41%, lr 1.000000e-06: 100%|██████████| 199/199 [01:29<00:00,  2.23it/s]\n",
            "04/20/2021 04:37:55 - valid loss: 0.30668\n",
            "04/20/2021 04:37:55 - valid f1 score: 64.42%\n",
            "04/20/2021 04:37:55 - valid recall: 72.39%\n",
            "04/20/2021 04:37:55 - valid precision: 68.73%\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pidTsbGNtp77"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCT3YaNatp9_"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWkr_vsTWyWU"
      },
      "source": [
        "## On Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VigQ7VwtqAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "173c6c7d-b5a7-4279-9732-d8daf531d82a"
      },
      "source": [
        "model.load_state_dict(torch.load('models/bert_model.pt'))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "SM9r3TrRW1_k",
        "outputId": "5cd80ff9-5186-47e2-a5d3-e2c4172d0f1a"
      },
      "source": [
        "tsd_test = pd.read_csv(\"toxic_spans/data/tsd_test.csv\") \n",
        "tsd_test.text = tsd_test.text.apply(lambda x:x.translate(translationTable))\n",
        "tsd_test.spans = tsd_test.spans.apply(literal_eval)\n",
        "tsd_test.tail(5)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>spans</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>[4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...</td>\n",
              "      <td>hey loser change your name to something more a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>[23, 24, 25, 26, 27]</td>\n",
              "      <td>And you are a complete moron who obviously doe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>[157, 158, 159, 160, 161, 162, 163, 164, 165, ...</td>\n",
              "      <td>Such vitriol from the left.  Who would have th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>[]</td>\n",
              "      <td>It is now time for most of you to expand your ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>[828, 829, 830, 831]</td>\n",
              "      <td>Why does this author think she can demand, or ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  spans                                               text\n",
              "1995  [4, 5, 6, 7, 8, 70, 71, 72, 73, 74, 75, 76, 77...  hey loser change your name to something more a...\n",
              "1996                               [23, 24, 25, 26, 27]  And you are a complete moron who obviously doe...\n",
              "1997  [157, 158, 159, 160, 161, 162, 163, 164, 165, ...  Such vitriol from the left.  Who would have th...\n",
              "1998                                                 []  It is now time for most of you to expand your ...\n",
              "1999                               [828, 829, 830, 831]  Why does this author think she can demand, or ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fRVTKdWW2C9",
        "outputId": "a2dc034e-b413-4c2a-df43-420c61daf389"
      },
      "source": [
        "text_list_test = tsd_test.text.to_list()\n",
        "spans_list_test = tsd_test.spans.to_list()\n",
        "print(len(text_list_test))\n",
        "print(len(spans_list_test))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000\n",
            "2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Xtsw5sftZhW"
      },
      "source": [
        "def predict(text,spans):\n",
        "    input_ids,attention_mask,labels = encode_and_trans_labels([text], [spans])\n",
        "    outputs = model(input_ids.to(device), attention_mask.to(device), labels=labels.to(device).long())\n",
        "    logits = outputs.logits \n",
        "    spans_pred = decode_and_trans_labels([text], input_ids, logits)[0]\n",
        "    gold = \"\".join(text[i] for i in spans)\n",
        "    pred = \"\".join(text[i] for i in spans_pred)\n",
        "    # print(\"gold: {}\\npred: {}\".format(gold,pred))\n",
        "    f1_score = f1(spans_pred, spans)\n",
        "    recall_score = recall(spans_pred, spans)\n",
        "    precision_score = precision(spans_pred, spans)\n",
        "    # print(\"f1 {:.2f}%, recall {:.2f}%, precision {:.2f}%,\".format(f1_score*100,recall_score*100,precision_score*100))\n",
        "    return f1_score,recall_score,precision_score"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BCuXUUPtZjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e992776-41eb-4959-d1a3-030d8ca682ad"
      },
      "source": [
        "f1_scores,recall_scores,precision_scores = [],[],[]\n",
        "for i in range(len(text_list_test)):\n",
        "    f1_score,recall_score,precision_score = predict(text_list[i],spans_list[i])\n",
        "    f1_scores.append(f1_score)\n",
        "    recall_scores.append(recall_score)\n",
        "    precision_scores.append(precision_score)\n",
        "print(\"All: f1 {:.2f}%, recall {:.2f}%, precision {:.2f}%,\".format(np.mean(f1_scores)*100,np.mean(recall_scores)*100,np.mean(precision_scores)*100))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All: f1 67.50%, recall 74.65%, precision 70.95%,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE-ja7Kmx_kf"
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUojR4tvx_mq"
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": []
    }
  ]
}